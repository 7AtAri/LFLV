{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers datasets\n",
    "#!pip install --upgrade huggingface hub\n",
    "#!pip install --upgrade pip\n",
    "#!pip install --upgrade torch torchvision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using feature embeddings to find new architectural concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to test two models for this: DINOv2 and CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DINOv2 Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, Dinov2Model\n",
    "import torch, torchvision\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n",
      "0.16.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available()  \n",
    "                            else \"mps\"  if torch.backends.mps.is_available() \n",
    "                            else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# If you have a model, move it to the device\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/ari/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "#!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ari/Documents/Data_Science/3_semester/learning_from_las_vegas/LFLV/LasVegas/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for huggingface/cats-image contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/huggingface/cats-image\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 100%|██████████| 2.56k/2.56k [00:00<00:00, 8.21MB/s]\n",
      "Downloading data: 100%|██████████| 173k/173k [00:00<00:00, 804kB/s] \n",
      "Generating test split: 1 examples [00:00,  2.24 examples/s]\n",
      "preprocessor_config.json: 100%|██████████| 436/436 [00:00<00:00, 2.73MB/s]\n",
      "config.json: 100%|██████████| 548/548 [00:00<00:00, 1.66MB/s]\n",
      "model.safetensors: 100%|██████████| 346M/346M [00:28<00:00, 11.9MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 257, 768]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "model = Dinov2Model.from_pretrained(\"facebook/dinov2-base\")\n",
    "\n",
    "inputs = image_processor(image, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### last hidden state shape = feature embeddings per image patch\n",
    "\n",
    "* 1 -> input batch size\n",
    "* 257 -> 256 image patches the input images are split into (one additional token for a special purpose, like classification or a start/end token). \n",
    "* 768 -> This is the dimensionality of the embeddings. Each of the 257 elements (patches or tokens) is transformed into a 768-dimensional vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.1747, -0.4729,  1.0936,  ...,  0.2041,  1.1101,  0.1363],\n",
       "         [-3.2780, -0.8269, -0.9210,  ...,  1.4415, -0.5364, -0.8757],\n",
       "         [-2.9129,  1.1284, -0.7306,  ...,  0.6959, -1.8791, -2.3638],\n",
       "         ...,\n",
       "         [-0.5463,  1.4382, -0.2563,  ...,  0.1873, -2.9950,  0.4067],\n",
       "         [-3.0848,  2.0568,  1.5137,  ...,  0.9157, -2.7059,  2.2017],\n",
       "         [-0.7499,  0.0903,  1.3731,  ..., -0.2961, -2.3682, -0.1329]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Clip model outputs just one feature vector per image, because it has already aggregated the patch embeddings. Therefore this is the simpler approach. The Clip model is a little bit older than DINOv2, but also incorporates rich and complex world knowledge due to the unsupervised training approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "\n",
    "# Preprocess the image\n",
    "image = Image.open(\"path_to_your_image.jpg\")\n",
    "image = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Calculate image features\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "\n",
    "# Now you can use the image_features for your application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### openCLIP from Laion:\n",
    "\n",
    "alternative to the older OpenAI clip model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/mlfoundations/open_clip.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "\n",
    "# Load the OpenCLIP model variant \"H/14\"\n",
    "model, preprocess = open_clip.create_model_and_transforms('H/14', pretrained='laion2b_s32b_b79k')\n",
    "\n",
    "# Preprocess the image\n",
    "image_path = \"path_to_your_image.jpg\"\n",
    "image = preprocess(Image.open(image_path)).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Generate embeddings\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "\n",
    "# image_features now contains the embeddings for your image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectural epochs / styles dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/dumitrux/architectural-styles-dataset?resource=download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing for DINO:\n",
    "- common size with consistent aspect ratio\n",
    "- normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in ./LasVegas/lib/python3.11/site-packages (from opencv-python) (1.26.2)\n",
      "Using cached opencv_python-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl (35.4 MB)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.9.0.80\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Create a transformation pipeline to resize and crop the image, \n",
    "    then convert it to a tensor.\n",
    "\n",
    "    Args:\n",
    "    - target_size: A tuple (height, width) for the target size.\n",
    "\n",
    "    Returns:\n",
    "    - A composed transformation function.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),  # Resize the shorter edge to target_size, maintaining aspect ratio\n",
    "        transforms.CenterCrop(target_size),  # Crop the center of the image\n",
    "        transforms.ToTensor(),  # Convert to tensor\n",
    "    ])\n",
    "\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image_folder = \"path/to/your/images\"\n",
    "processed_images = []\n",
    "transform = transform_image(target_size=(224, 224))\n",
    "\n",
    "for image_name in os.listdir(image_folder):\n",
    "    image_path = os.path.join(image_folder, image_name)\n",
    "    image = Image.open(image_path).convert('RGB')  # Open image using PIL and convert to RGB\n",
    "    processed_image = transform(image)  # Apply the transformation\n",
    "    processed_images.append(processed_image)\n",
    "\n",
    "# processed_images now contains tensors of your images, preprocessed and ready for the model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LasVegas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
