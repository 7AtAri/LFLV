{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers datasets\n",
    "#!pip install --upgrade huggingface hub\n",
    "#!pip install --upgrade pip\n",
    "#!pip install --upgrade torch torchvision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using feature embeddings to find new architectural concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First get an example feature output from DINO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DINOv2 Model Example output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, Dinov2Model\n",
    "import torch, torchvision\n",
    "from datasets import load_dataset\n",
    "#from .autonotebook import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n",
      "0.16.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available()  \n",
    "                            else \"mps\"  if torch.backends.mps.is_available() \n",
    "                            else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# If you have a model, move it to the device\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/ari/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "#!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ari/Documents/Data_Science/3_semester/learning_from_las_vegas/LFLV/LasVegas/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for huggingface/cats-image contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/huggingface/cats-image\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 100%|██████████| 2.56k/2.56k [00:00<00:00, 8.21MB/s]\n",
      "Downloading data: 100%|██████████| 173k/173k [00:00<00:00, 804kB/s] \n",
      "Generating test split: 1 examples [00:00,  2.24 examples/s]\n",
      "preprocessor_config.json: 100%|██████████| 436/436 [00:00<00:00, 2.73MB/s]\n",
      "config.json: 100%|██████████| 548/548 [00:00<00:00, 1.66MB/s]\n",
      "model.safetensors: 100%|██████████| 346M/346M [00:28<00:00, 11.9MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 257, 768]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "model = Dinov2Model.from_pretrained(\"facebook/dinov2-base\")\n",
    "\n",
    "inputs = image_processor(image, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### last hidden state shape = feature embeddings per image patch\n",
    "\n",
    "* 1 -> input batch size\n",
    "* 257 -> 256 image patches the input images are split into (one additional token for a special purpose, like classification or a start/end token). \n",
    "* 768 -> This is the dimensionality of the embeddings. Each of the 257 elements (patches or tokens) is transformed into a 768-dimensional vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.1747, -0.4729,  1.0936,  ...,  0.2041,  1.1101,  0.1363],\n",
       "         [-3.2780, -0.8269, -0.9210,  ...,  1.4415, -0.5364, -0.8757],\n",
       "         [-2.9129,  1.1284, -0.7306,  ...,  0.6959, -1.8791, -2.3638],\n",
       "         ...,\n",
       "         [-0.5463,  1.4382, -0.2563,  ...,  0.1873, -2.9950,  0.4067],\n",
       "         [-3.0848,  2.0568,  1.5137,  ...,  0.9157, -2.7059,  2.2017],\n",
       "         [-0.7499,  0.0903,  1.3731,  ..., -0.2961, -2.3682, -0.1329]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectural epochs / styles dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "used hierarchical dataset:\n",
    "- https://www.kaggle.com/datasets/gustavoachavez/architectural-styles-periods-dataset\n",
    "\n",
    "flat dataset alternative:\n",
    "- https://www.kaggle.com/datasets/dumitrux/architectural-styles-dataset?resource=download\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing for DINO:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in ./LasVegas/lib/python3.11/site-packages (from opencv-python) (1.26.2)\n",
      "Using cached opencv_python-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl (35.4 MB)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.9.0.80\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, Dinov2Model\n",
    "import torch, torchvision\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available()  \n",
    "                            #else \"mps\"  if torch.backends.mps.is_available() \n",
    "                            else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# If you have a model, move it to the device\n",
    "# model.to(device)\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculating the centroid for all images in a folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "# Path to your folder\n",
    "folder_path = 'data/Architectural_Styles/21st_Century/Neo-Futurism'\n",
    "image_paths = glob.glob(folder_path + '/*.jpg')  # Adjust the pattern as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the processor and model\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "model = Dinov2Model.from_pretrained(\"facebook/dinov2-base\").to(device)\n",
    "\n",
    "mean_last_hidden_states = []  # Store means here\n",
    "\n",
    "for image_path in image_paths:\n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Process the image\n",
    "    inputs = image_processor(image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        \n",
    "    mean_output = output.last_hidden_state.mean(dim=1)\n",
    "    mean_last_hidden_states.append(mean_output)\n",
    "\n",
    "# Calculate centroid of the means\n",
    "#all_means_tensor = torch.stack(mean_last_hidden_states)\n",
    "#centroid = all_means_tensor.mean(dim=0)\n",
    "\n",
    "# Assuming mean_last_hidden_states is a list of tensors with shape [1, 768]\n",
    "# Stack all the mean last hidden state tensors along a new dimension\n",
    "all_means_tensor = torch.cat(mean_last_hidden_states, dim=0)\n",
    "\n",
    "# Calculate the centroid by computing the mean across the new batch dimension\n",
    "centroid = all_means_tensor.mean(dim=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_list = centroid.tolist()\n",
    "len(centroid_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### safe the centroid in a dictionary of embeddings and serialize it with json:\n",
    "\n",
    "- key = folder_name\n",
    "- value = centroid vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "# Example path\n",
    "path=Path(folder_path)\n",
    "\n",
    "# Extract the folder name\n",
    "folder_name = path.parts[-1]\n",
    "\n",
    "# save the centroid vector to a file\n",
    "# Path to the file\n",
    "file_path = 'embeddings_dict.json'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Load dictionary from file\n",
    "    with open(file_path, 'r') as file:\n",
    "        embeddings_dict = json.load(file)\n",
    "    print(\"Dictionary loaded successfully.\")\n",
    "else:\n",
    "    print(\"File does not exist.\")\n",
    "    embeddings_dict = {}  # Initialize an empty dictionary or handle the absence as needed\n",
    "\n",
    "# Convert global_centroid_vector to a list for JSON compatibility, if necessary\n",
    "centroid_list = centroid.tolist()\n",
    "\n",
    "# Save to dictionary\n",
    "embeddings_dict[folder_name] = centroid_list\n",
    "\n",
    "# Save dictionary to file\n",
    "with open('embeddings_dict.json', 'w') as file:\n",
    "    json.dump(embeddings_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Blobitecture', 'Deconstructivism', 'Eco-architecture', 'Neo-Futurism'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dict = embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# save the centroid vector to a file\n",
    "# Path to the file\n",
    "file_path_top = 'architecture_dict.json'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path_top):\n",
    "    # Load dictionary from file\n",
    "    with open(file_path_top, 'r') as file:\n",
    "        architecture_dict = json.load(file)\n",
    "    print(\"Dictionary loaded successfully.\")\n",
    "else:\n",
    "    print(\"File does not exist.\")\n",
    "    architecture_dict = {}  # Initialize an empty dictionary or handle the absence as needed\n",
    "\n",
    "\n",
    "# Specify the new top-level key\n",
    "top_level_key = \"21st_Century\"\n",
    "\n",
    "# Calculate the mean vector of the existing values\n",
    "mean_vector = np.mean(list(current_dict.values()), axis=0)\n",
    "\n",
    "folder_name_top = \"21st_Century\"\n",
    "# Save to dictionary\n",
    "architecture_dict[folder_name_top] =   {\"subcategories\": current_dict,\n",
    "                                                                 \"mean_vector\": mean_vector.tolist() }\n",
    "\n",
    "\n",
    "# Save dictionary to file\n",
    "with open('architecture_dict.json', 'w') as file:\n",
    "    json.dump(architecture_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['21st_Century'])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "architecture_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21st_Century:\n",
      "    subcategories:\n",
      "        Blobitecture\n",
      "        Deconstructivism\n",
      "        Eco-architecture\n",
      "        Neo-Futurism\n",
      "    mean_vector\n"
     ]
    }
   ],
   "source": [
    "def print_keys_with_indent(dictionary, indent=0):\n",
    "    for key, value in dictionary.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(\" \" * indent + key + \":\")\n",
    "            print_keys_with_indent(value, indent + 4)  # Increase indentation for nested dictionaries\n",
    "        else:\n",
    "            print(\" \" * indent + key)\n",
    "\n",
    "# Call the function to print keys with indentation\n",
    "print_keys_with_indent(architecture_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize image embeddings with thumbnails of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you have:\n",
    "\n",
    "- projected_embeddings: A NumPy array of shape (n_images, 2) with your 2D projected embeddings.\n",
    "- image_paths: A list of file paths corresponding to the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thumbnail_paths = []\n",
    "thumbnail_size = (100, 100)\n",
    "\n",
    "for path in image_paths:\n",
    "    img = Image.open(path)\n",
    "    img.thumbnail(thumbnail_size)\n",
    "    thumbnail_path = f'thumbnail_{path}'\n",
    "    img.save(thumbnail_path)\n",
    "    thumbnail_paths.append(thumbnail_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses customdata to store the path to the thumbnail images and displays them using the hover template. Plotly's hover templates allow HTML content, which is how the images are displayed.\n",
    "Click events in Plotly typically require JavaScript or Dash (a Python framework for building web applications) for more complex interactions. The example above uses hover actions for simplicity, as true click events to display images would necessitate a more complex setup, likely involving a web server or Dash app.\n",
    "Ensure that the paths to the thumbnails are accessible from where the Plotly figure is being viewed. If you're viewing the figure in a Jupyter notebook, the image paths may need to be relative to the notebook or served through a web server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create a scatter plot\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=projected_embeddings[:, 0], \n",
    "    y=projected_embeddings[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5),\n",
    "    customdata=thumbnail_paths,\n",
    "    hoverinfo='none'\n",
    ")])\n",
    "\n",
    "# Update layout for hover functionality\n",
    "fig.update_layout(\n",
    "    hovermode='closest',\n",
    "    title=\"Image Embeddings Visualization\"\n",
    ")\n",
    "\n",
    "# Use JavaScript for custom hover functionality to display images\n",
    "fig.update_traces(\n",
    "    hovertemplate='<img src=\"%{customdata}\"></img>',\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternatively:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def image_path_to_data_uri(path):\n",
    "    with open(path, \"rb\") as image_file:\n",
    "        encoded_string = base64.b64encode(image_file.read()).decode()\n",
    "    return f\"data:image/png;base64,{encoded_string}\"\n",
    "\n",
    "# Convert all thumbnail paths to data URIs\n",
    "data_uris = [image_path_to_data_uri(path) for path in thumbnail_paths]\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=projected_embeddings[:, 0], \n",
    "    y=projected_embeddings[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5),\n",
    "    customdata=data_uris,\n",
    "    hoverinfo='none'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    hovermode='closest',\n",
    "    title=\"Image Embeddings Visualization\"\n",
    ")\n",
    "\n",
    "# Update hover template to use data URIs\n",
    "fig.update_traces(\n",
    "    hovertemplate='<img src=\"%{customdata}\"></img>',\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LasVegas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
