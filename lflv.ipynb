{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers datasets\n",
    "#!pip install --upgrade huggingface hub\n",
    "#!pip install --upgrade pip\n",
    "#!pip install --upgrade torch torchvision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using feature embeddings to find new architectural concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to test two models for this: DINOv2 and CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DINOv2 Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, Dinov2Model\n",
    "import torch, torchvision\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n",
      "0.16.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available()  \n",
    "                            else \"mps\"  if torch.backends.mps.is_available() \n",
    "                            else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# If you have a model, move it to the device\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/ari/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "#!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ari/Documents/Data_Science/3_semester/learning_from_las_vegas/LFLV/LasVegas/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for huggingface/cats-image contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/huggingface/cats-image\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 100%|██████████| 2.56k/2.56k [00:00<00:00, 8.21MB/s]\n",
      "Downloading data: 100%|██████████| 173k/173k [00:00<00:00, 804kB/s] \n",
      "Generating test split: 1 examples [00:00,  2.24 examples/s]\n",
      "preprocessor_config.json: 100%|██████████| 436/436 [00:00<00:00, 2.73MB/s]\n",
      "config.json: 100%|██████████| 548/548 [00:00<00:00, 1.66MB/s]\n",
      "model.safetensors: 100%|██████████| 346M/346M [00:28<00:00, 11.9MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 257, 768]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "model = Dinov2Model.from_pretrained(\"facebook/dinov2-base\")\n",
    "\n",
    "inputs = image_processor(image, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### last hidden state shape = feature embeddings per image patch\n",
    "\n",
    "* 1 -> input batch size\n",
    "* 257 -> 256 image patches the input images are split into (one additional token for a special purpose, like classification or a start/end token). \n",
    "* 768 -> This is the dimensionality of the embeddings. Each of the 257 elements (patches or tokens) is transformed into a 768-dimensional vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.1747, -0.4729,  1.0936,  ...,  0.2041,  1.1101,  0.1363],\n",
       "         [-3.2780, -0.8269, -0.9210,  ...,  1.4415, -0.5364, -0.8757],\n",
       "         [-2.9129,  1.1284, -0.7306,  ...,  0.6959, -1.8791, -2.3638],\n",
       "         ...,\n",
       "         [-0.5463,  1.4382, -0.2563,  ...,  0.1873, -2.9950,  0.4067],\n",
       "         [-3.0848,  2.0568,  1.5137,  ...,  0.9157, -2.7059,  2.2017],\n",
       "         [-0.7499,  0.0903,  1.3731,  ..., -0.2961, -2.3682, -0.1329]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Clip model outputs just one feature vector per image, because it has already aggregated the patch embeddings. Therefore this is the simpler approach. The Clip model is a little bit older than DINOv2, but also incorporates rich and complex world knowledge due to the unsupervised training approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "\n",
    "# Preprocess the image\n",
    "image = Image.open(\"path_to_your_image.jpg\")\n",
    "image = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Calculate image features\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "\n",
    "# Now you can use the image_features for your application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### openCLIP from Laion:\n",
    "\n",
    "alternative to the older OpenAI clip model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/mlfoundations/open_clip.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "\n",
    "# Load the OpenCLIP model variant \"H/14\"\n",
    "model, preprocess = open_clip.create_model_and_transforms('H/14', pretrained='laion2b_s32b_b79k')\n",
    "\n",
    "# Preprocess the image\n",
    "image_path = \"path_to_your_image.jpg\"\n",
    "image = preprocess(Image.open(image_path)).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Generate embeddings\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "\n",
    "# image_features now contains the embeddings for your image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectural epochs / styles dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/dumitrux/architectural-styles-dataset?resource=download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing for DINO:\n",
    "- common size with consistent aspect ratio\n",
    "- normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in ./LasVegas/lib/python3.11/site-packages (from opencv-python) (1.26.2)\n",
      "Using cached opencv_python-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl (35.4 MB)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.9.0.80\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ari/Documents/Data_Science/3_semester/learning_from_las_vegas/LFLV/LasVegas/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Create a transformation pipeline to resize and crop the image, \n",
    "    then convert it to a tensor.\n",
    "\n",
    "    Args:\n",
    "    - target_size: A tuple (height, width) for the target size.\n",
    "\n",
    "    Returns:\n",
    "    - A composed transformation function.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),  # Resize the shorter edge to target_size, maintaining aspect ratio\n",
    "        transforms.CenterCrop(target_size),  # Crop the center of the image\n",
    "        transforms.ToTensor(),  # Convert to tensor\n",
    "    ])\n",
    "\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image_folder = \"path/to/your/images\"\n",
    "processed_images = []\n",
    "transform = transform_image(target_size=(224, 224))\n",
    "\n",
    "for image_name in os.listdir(image_folder):\n",
    "    image_path = os.path.join(image_folder, image_name)\n",
    "    image = Image.open(image_path).convert('RGB')  # Open image using PIL and convert to RGB\n",
    "    processed_image = transform(image)  # Apply the transformation\n",
    "    processed_images.append(processed_image)\n",
    "\n",
    "# processed_images now contains tensors of your images, preprocessed and ready for the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, Dinov2Model\n",
    "import torch, torchvision\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available()  \n",
    "                            else \"mps\"  if torch.backends.mps.is_available() \n",
    "                            else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# If you have a model, move it to the device\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "model = Dinov2Model.from_pretrained(\"facebook/dinov2-base\")\n",
    "\n",
    "inputs = image_processor(image, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataloading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (224, 224)  # The target size for the images (height, width) for DINO input\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(target_size),  # Resize the shorter edge to target_size, maintaining aspect ratio\n",
    "    transforms.CenterCrop(target_size),  # Crop the center of the image\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) check number of workers available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_num_workers = os.cpu_count()\n",
    "max_num_workers//2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Assuming `image_folder` is the path to your image directory\n",
    "dataset = CustomImageDataset(image_folder, transform=transform)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=max_num_workers//2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with batch processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize a variable to hold the sum of all embeddings and a counter for the number of images\n",
    "sum_embeddings = torch.zeros(768)\n",
    "total_images = 0\n",
    "\n",
    "for images in dataloader:  # Assuming `dataloader` yields batches of images\n",
    "    # Process images through the model to get embeddings\n",
    "    # Ensure your model and images are on the same device (e.g., CPU or GPU)\n",
    "    images = images.to(device)  # Move images to the same device as the model\n",
    "    outputs = model(images)  # Get model outputs, adjust this line according to your model\n",
    "    \n",
    "    # Assuming `outputs` is now a tensor of shape [batch_size, 257, 768]\n",
    "    # Calculate the mean across the patch dimension, including the CLS token\n",
    "    batch_mean = outputs.mean(dim=(0, 1))  # Mean over batch and patch dimensions\n",
    "    \n",
    "    # Adjust for the batch size\n",
    "    sum_embeddings += batch_mean * outputs.shape[0]  # Multiply by batch size and add to sum\n",
    "    total_images += outputs.shape[0]  # Accumulate the total number of images processed\n",
    "\n",
    "# Calculate the global centroid vector\n",
    "global_centroid_vector = sum_embeddings / total_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### in case I would just input one giant batch of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example tensor `outputs` with shape [batch_size, 257, 768]\n",
    "# outputs = model_output\n",
    "\n",
    "# Calculate the mean over both the batch and patch dimensions to get the centroid\n",
    "centroid_vector = outputs.mean(dim=(0, 1))  # This averages across all images and all patches\n",
    "\n",
    "# `centroid_vector` will have the shape [768], representing the global centroid of the embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LasVegas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
